{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEI2jCnX6sfdAng++W8qn+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import keras\n","from keras import layers\n","from matplotlib import pyplot as plt\n","\n","# Load the data\n","nab_data_url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realTweets/Twitter_volume_IBM.csv\"\n","df_twitter_volume = pd.read_csv(nab_data_url, parse_dates=True, index_col=\"timestamp\")\n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(df_twitter_volume.index, df_twitter_volume[\"value\"], label=\"Twitter Volume\")\n","plt.xlabel(\"Timestamp\")\n","plt.ylabel(\"Volume\")\n","plt.title(\"Twitter Volume Data for IBM\")\n","plt.legend()\n","plt.show()\n","\n","# Normalize the training data\n","training_mean = df_twitter_volume[\"value\"].mean()\n","training_std = df_twitter_volume[\"value\"].std()\n","df_training_value = (df_twitter_volume[\"value\"] - training_mean) / training_std\n","print(\"Number of training samples:\", len(df_training_value))\n","\n","# Create sequences (adjust sequence length as needed)\n","sequence_length = 288  # 288 timesteps per day\n","sequences = []\n","for i in range(len(df_training_value) - sequence_length):\n","    sequences.append(df_training_value[i : i + sequence_length])\n","\n","# Convert sequences to numpy array\n","X_train = np.array(sequences)\n","\n","# Define the Autoencoder architecture\n","input_dim = X_train.shape[1]\n","encoding_dim = 64\n","\n","autoencoder = keras.Sequential([\n","    layers.Input(shape=(input_dim,)),\n","    layers.Dense(encoding_dim, activation=\"relu\"),\n","    layers.Dense(input_dim, activation=\"linear\")\n","])\n","\n","# Compile the model\n","autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n","\n","# Train the Autoencoder\n","history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=64, validation_split=0.1, shuffle=True)\n","\n","# Plot training loss\n","plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n","plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Autoencoder Training Loss\")\n","plt.legend()\n","plt.show()\n","\n","# Detect anomalies\n","reconstructed_X = autoencoder.predict(X_train)\n","mse = np.mean(np.power(X_train - reconstructed_X, 2), axis=1)\n","threshold = np.percentile(mse, 95)  # Set a threshold for anomaly detection\n","\n","# Identify anomalies\n","anomalies = df_twitter_volume.iloc[sequence_length:][mse > threshold]\n","\n","# Visualize anomalies\n","plt.figure(figsize=(12, 6))\n","plt.plot(df_twitter_volume.index, df_twitter_volume[\"value\"], label=\"Twitter Volume\")\n","plt.scatter(anomalies.index, anomalies[\"value\"], color=\"red\", label=\"Anomalies\")\n","plt.xlabel(\"Timestamp\")\n","plt.ylabel(\"Volume\")\n","plt.title(\"Twitter Volume Anomalies for IBM\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"aKzNMP5CXip5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anomalies.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4exwN-rcBDVy","executionInfo":{"status":"ok","timestamp":1713216181646,"user_tz":240,"elapsed":124,"user":{"displayName":"Udayveer Singh Andotra","userId":"04950604640035593955"}},"outputId":"fefc2803-b6bf-4d45-8f05-73ddea8e5362"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(781, 1)"]},"metadata":{},"execution_count":21}]}]}